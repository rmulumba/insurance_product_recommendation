{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from random import seed\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time"
   ]
  },
  {
   "source": [
    "### Loading the datasets and setting relevant parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  pd.read_csv(\"../input/Train.csv\")\n",
    "test = pd.read_csv(\"../input/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"../input/SampleSubmission.csv\")\n",
    "ss.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette(\"Blues\", desat=0.45)\n",
    "plt.figure(figsize=(20, 12))\n",
    "matrix = np.triu(train.corr())\n",
    "\n",
    "sns.heatmap(train.corr(), annot=True, mask=matrix, cmap=cmap, cbar=False)\n",
    "plt.title(\"Train set correlation matrix\", fontdict={'fontsize':20})\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Creating a dataset with all the products labeled 1 in the test set in a format similar to the submission file <br>\"IP X CODE\" column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_values = []\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    customer_metadata = row[:8]\n",
    "    product_labels = row[8:]\n",
    "    selected_labels = [index for index, product_label  in enumerate(product_labels) if product_label == 1]\n",
    "    \n",
    "    for val in test.columns[8:][selected_labels]:\n",
    "        test_true_values.append(row[0] + ' X ' + val)\n",
    "print(len(test_true_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_true_values[:10])"
   ]
  },
  {
   "source": [
    "### Converting the multilabel dataset into a single label dataset using the copy method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_to_singlelabel(dataset, first_multilabel_col):\n",
    "    single_label = []\n",
    "    row_counter = 0\n",
    "    data_columns = dataset.columns\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        customer_metadata = row[:first_multilabel_col]\n",
    "        product_labels = row[first_multilabel_col:]\n",
    "        selected_labels = [index for index, product_label  in enumerate(product_labels) if product_label == 1]\n",
    "\n",
    "        for selected_label in selected_labels:\n",
    "            row_counter += 1\n",
    "            for product_label in range(len(product_labels)):\n",
    "                if product_label == selected_label:\n",
    "                    transformed_labels = list(copy.copy(product_labels))\n",
    "                    transformed_labels[selected_label] = 0\n",
    "                    single_label.append(list(customer_metadata) + transformed_labels\n",
    "                                        + [data_columns[first_multilabel_col+product_label]])\n",
    "                    \n",
    "    single_label = pd.DataFrame(single_label)\n",
    "    single_label.columns = ['ID', 'join_date', 'sex', 'marital_status', 'birth_year', 'branch_code', \n",
    "                            'occupation_code', 'occupation_category_code', 'P5DA', 'RIBP', '8NN1', '7POT', \n",
    "                            '66FJ', 'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9', 'N2MW', 'AHXO', 'BSTQ', 'FM3X', \n",
    "                            'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW', 'GHYX', 'ECY3', 'new_prediction']\n",
    "    \n",
    "    return single_label"
   ]
  },
  {
   "source": [
    "### Train Dataset transformed to single labels using the copy-weight method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = multilabel_to_singlelabel(train, 8)\n",
    "display(train.head())"
   ]
  },
  {
   "source": [
    "### Test Dataset tranformed to single labels using the copy-weight method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "display(test.head())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Feature engineering<br>\n",
    "### Converting the date into separate day, month and year columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(data_frame):\n",
    "    data_frame['join_date'] = pd.to_datetime(data_frame['join_date'], format=\"%d/%m/%Y\")\n",
    "    data_frame['year_joined'],data_frame['month_joined'],data_frame['day_joined'] = (data_frame.join_date.dt.year, \n",
    "                                                                                    data_frame.join_date.dt.month, \n",
    "                                                                                    data_frame.join_date.dt.day)\n",
    "    data_frame = data_frame.drop(columns=['join_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformed train dataset\")\n",
    "transform_date(train)\n",
    "display(train.head())\n",
    "print(\"Transformed test dataset\")\n",
    "transform_date(test)\n",
    "display(test.head())"
   ]
  },
  {
   "source": [
    "### Fixing missing values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train null values\\n')\n",
    "print(train.isnull().sum())\n",
    "print('Test null values\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "source": [
    "miss_val_list = ['year_joined', 'month_joined', 'day_joined']\n",
    "for val in miss_val_list:\n",
    "    train[val] = train[val].fillna(value=train[val].mean())\n",
    "    test[val] = test[val].fillna(value=test[val].mean())\n",
    "    \n",
    "print(\"Train set\\n\")\n",
    "print(train.isnull().sum())\n",
    "print(\"Test set\\n\")\n",
    "print(test.isnull().sum())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dtypes"
   ]
  },
  {
   "source": [
    "### Adding the age in which the customer joined"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_joined'] = train['year_joined'] - train['birth_year']\n",
    "test['age_joined'] = test['year_joined'] - test['birth_year']\n",
    "\n",
    "train['age'] = train['year_joined'] - train['birth_year']\n",
    "test['age'] = test['year_joined'] - test['birth_year']"
   ]
  },
  {
   "source": [
    "### Adding the number of years as a customer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['years_as_customer'] = pd.to_datetime('now').year - train['year_joined']\n",
    "test['years_as_customer'] = pd.to_datetime('now').year - test['year_joined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set head')\n",
    "display(train.head())\n",
    "print('Testing set head')\n",
    "display(test.head())"
   ]
  },
  {
   "source": [
    "### Encoding categorical data\n",
    "#### Finding differences in values between the trainining set and the testing set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_encode = ['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']\n",
    "dataframes = [train, test]\n",
    "print(\"Differences in categorical values between the train set and the test set.\\n\")\n",
    "for l in list_to_encode:\n",
    "    print(l, \":\", list(set(test[l]).difference(train[l])) if set(test[l]).difference(train[l]) else 0)\n",
    "    print(\"-------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "source": [
    "The categories sex, marital_status, branch_code, occupation_code and occupation_category_code have values in the train set that are absent in the testing set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Combining the two datasets and the doing encoding on the combined dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['train'] = 1\n",
    "test['train'] = 0\n",
    "\n",
    "combined = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_one_hot = pd.get_dummies(combined, )\n",
    "list_to_encode = ['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code']\n",
    "\n",
    "combined_one_hot = combined[list_to_encode]\n",
    "combined_one_hot = pd.get_dummies(combined_one_hot)\n",
    "combined = combined.drop(columns=list_to_encode)\n",
    "combined = pd.concat([combined, combined_one_hot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = combined[combined['train'] == 1]\n",
    "test = combined[combined['train'] == 0]\n",
    "\n",
    "train.drop(columns=['train'], axis =1, inplace=True)\n",
    "test.drop(columns=['train'], axis =1, inplace=True)\n",
    "\n",
    "display(train.tail(10))\n",
    "display(test.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(train[['new_prediction']])\n",
    "features = train.drop(columns=['new_prediction', 'ID'])\n",
    "test =test.drop(columns=['new_prediction'])\n",
    "\n",
    "display(features.tail())\n",
    "display(target.head())\n",
    "display(test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "target.loc[:,'new_prediction'] = le.fit_transform(target.loc[:,'new_prediction'])"
   ]
  },
  {
   "source": [
    "### Creating the training and validation sets "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_valid_pool(features, target):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.15, random_state=42)\n",
    "    train_pool = Pool(features, target)\n",
    "    val_pool = Pool(X_val, y_val) \n",
    "    return train_pool, val_pool\n",
    "\n",
    "def create_train_valid_set(features, target):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.25, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "train_pool, val_pool= create_train_valid_pool(features, target)\n",
    "X_train, X_val, y_train, y_val = create_train_valid_set(features, target)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "clf1 = CatBoostClassifier(\n",
    "    iterations=5000, # 10000, \n",
    "    random_state=42, \n",
    "    learning_rate=0.0245, \n",
    "    task_type='GPU', \n",
    "    devices='0', \n",
    "    verbose=True\n",
    ")\n",
    "model = clf1.fit(train_pool, eval_set=val_pool, plot=False)\n",
    "print('Total training time:',(time.time() - start_time)/60, 'minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.get_feature_importance(train_pool)\n",
    "feature_names = X_train.columns\n",
    "fi_names=[]\n",
    "\n",
    "for score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n",
    "    fi_names.append([name] + [score])\n",
    "    #print(\"{},{}\".format(name, score))\n",
    "fi = pd.DataFrame(fi_names)\n",
    "fi.columns = ['Feature', 'Score']\n",
    "\n",
    "low_fi_list = fi[fi['Score'] <= 0.000]\n",
    "low_fi_list = list(low_fi_list.drop(columns=['Score']).values.flatten())\n",
    "#display(low_fi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_fi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low_fi_list.reset_index(drop=True, inplace=True)\n",
    "fi = pd.DataFrame(fi)\n",
    "fi.to_csv('fi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = features.drop(columns=low_fi_list)\n",
    "test_2 = test.drop(columns=low_fi_list)\n",
    "\n",
    "train_pool, val_pool = create_train_valid_pool(features_2, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf1 = CatBoostClassifier(\n",
    "    iterations=5000, # 10000, \n",
    "    random_state=42, \n",
    "    learning_rate=0.0245, \n",
    "    task_type='GPU', \n",
    "    devices='0', \n",
    "    verbose=True\n",
    ")\n",
    "model = clf1.fit(train_pool, eval_set=val_pool, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_2\n",
    "proba = model.predict_proba(X_test.drop(columns=['ID'], axis=1))\n",
    "y_test = pd.DataFrame(proba)\n",
    "y_test.columns = le.inverse_transform(y_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "source": [
    "### Creating the submission file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "test = test_2\n",
    "for row in tqdm(range(y_test.shape[0])):\n",
    "    ID = test['ID'].iloc[row]\n",
    "    for column in y_test.columns:\n",
    "        prediction_list.append([ID + ' X ' + column, y_test[column].iloc[row]])       \n",
    "        \n",
    "prediction_df = pd.DataFrame(prediction_list)\n",
    "prediction_df.columns = ['ID X PCODE', 'Label']\n",
    "\n",
    "cleaned_data = [['ID X PCODE', 'Label']]\n",
    "for [code, label] in tqdm(prediction_df.values):\n",
    "    cleaned_label = 1.0 if code in test_true_values else label\n",
    "    cleaned_data.append([code, cleaned_label])\n",
    "    \n",
    "headers = cleaned_data.pop(0)\n",
    "cleaned_prediction_df = pd.DataFrame(cleaned_data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_prediction_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_prediction_df.reset_index(drop=True, inplace=True)\n",
    "cleaned_prediction_df.to_csv('submission_final_84.csv', index=False)\n",
    "\n",
    "print('Total training time:',(time.time() - start_time)/60, 'minutes.')"
   ]
  },
  {
   "source": [
    "### References <br>\n",
    "1. Gibaja, Eva & Ventura, Sebastian. (2015). A Tutorial on Multi-Label Learning. ACM Computing Surveys. 47.10.1145/2716262. <br>\n",
    "https://www.researchgate.net/publication/270337594_A_Tutorial_on_Multi-Label_Learning\n",
    "2. Modelling tabular data with CatBoost and NODE<br>\n",
    "https://towardsdatascience.com/modelling-tabular-data-with-catboost-and-node-929bfbaaeb08\n",
    "3. Zimnat Recommendation Challenge<br>\n",
    "https://github.com/Tixonmavrin/Zindi-Zimnat-Insurance-Recommendation-Challenge/blob/master/Baseline1.ipynb\n",
    "4. Exploring Embeddings for Categorical Variables with Keras<br>\n",
    "http://flovv.github.io/Embeddings_with_keras/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}